# ğŸŒ³ Heart Disease Prediction using Decision Tree Classifier

This project aims to predict whether a patient is likely to develop heart disease based on medical and demographic features. The model used is a Decision Tree Classifier, which is highly interpretable and effective for classification problems.

---

## ğŸ“ Project Overview

- ğŸ“Š Problem Type: Binary Classification  
- ğŸ¯ Objective: Predict the presence (1) or absence (0) of heart disease  
- ğŸ“‚ Dataset: heart.csv (UCI Heart Disease Dataset)

---

## ğŸ” Features in Dataset

| Feature     | Description                                      |
|-------------|--------------------------------------------------|
| age         | Age of the patient                               |
| sex         | Gender (0 = female, 1 = male)                    |
| cp          | Chest pain type (categorical: 0â€“3)              |
| trestbps    | Resting blood pressure                           |
| chol        | Serum cholesterol level                          |
| fbs         | Fasting blood sugar > 120 mg/dl (1 = true, 0 = false) |
| restecg     | Resting ECG results                              |
| thalach     | Maximum heart rate achieved                      |
| exang       | Exercise-induced angina (1 = yes, 0 = no)       |
| oldpeak     | ST depression induced by exercise                |
| slope       | Slope of peak exercise ST segment                |
| ca          | Number of major vessels (0â€“3)                    |
| thal        | Thalassemia (0 = normal, 1 = fixed, 2 = reversible) |
| target      | Diagnosis of heart disease (1 = disease, 0 = no disease) |

---

## ğŸ”§ Steps Performed

1. âœ… Loaded and explored dataset (703 rows Ã— 14 columns)  
2. âœ… Verified no missing/null values  
3. âœ… Dropped 1 duplicate row  
4. âœ… No encoding needed (dataset already numerical)  
5. âœ… Split data into 80% training and 20% testing  
6. âœ… Built Decision Tree Classifier using scikit-learn  
7. âœ… Tuned max_depth parameter (1â€“10) to prevent overfitting  
8. âœ… Visualized decision tree structure and feature splits

---

## ğŸ“ˆ Model Evaluation

| Metric           | Score         |
|------------------|---------------|
| Accuracy Score   | 80.32%        |
| Best max_depth   | 8             |
| Confusion Matrix | [[TN, FP], [FN, TP]] format included in notebook |

âœ… Interpretation: A max_depth of 8 gave the best performance, balancing bias and variance.

---

## ğŸ“Š Visualization

- Decision Tree plotted using plot_tree  
- Feature importance ranking  
- Correlation heatmap (EDA)

---

## ğŸ§  Key Learnings

- Decision Trees split data based on feature purity (homogeneity)  
- Learned about ID3 and CART algorithms (Information Gain vs Gini Index)  
- Understood pruning (max_depth) as a method to reduce overfitting  
- Visualizing the tree makes it easy to interpret how decisions are made

---

## ğŸ“š Tools & Libraries

- Python 3  
- pandas, numpy  
- matplotlib, seaborn  
- scikit-learn

---

## ğŸ™‹â€â™€ï¸ About Me

Hi, Iâ€™m Ruthika Nalajala â€” an aspiring Machine Learning Engineer on a mission to build impactful AI solutions. This project is part of my ML journey through IntelPath and personal practice.

ğŸ“¬ Connect with me on [LinkedIn](https://www.linkedin.com/in/ruthika-nalajala-73127628b/)

---

â­ Feel free to fork this repo, explore the notebook, and suggest improvements. Thanks for visiting!
