# -*- coding: utf-8 -*-
"""LogisticRegression_bankingproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YN8LG9lJQHlA9NMQuTM2leiJ1I5Ue-b6

#**Importing libraries**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.read_csv('/content/bank-additional-dataset.csv')
df

"""#**EDA**"""

#steps in EDA
#1.null values
#2.duplicate values
#3.outliers
#4.label encoding

df.info()

df.shape

df['y'].value_counts()

"""**Handling null values**"""

df.isnull().sum().sum()
#if you need to drop the null values:
#df.dropna(inplace=True)
#if you want to fill the null values:
#use fillna()

"""**Handling Duplicate values**"""

df.duplicated().sum()

#To check the duplicate values
df[df.duplicated()==True]

#removing dupliacte values from dataset
df.drop_duplicates(inplace=True)
df.duplicated().sum()

#df after removing duplicates
df

"""**Outliers**"""

df.columns

#outlier detection for all the columns in the dataset

for x in df.columns:
  if df[x].dtype=='int64' or df[x].dtype=='float64':
    sns.boxplot(df[x])
    plt.xlabel(x)
    plt.show()

out_list=['age','campaign','cons.conf.idx']

for x in out_list:
  Q1=df[x].quantile(0.25)
  Q3=df[x].quantile(0.75)
  IQR=Q3-Q1
  lower_bound=Q1-1.5*IQR
  upper_bound=Q3+1.5*IQR
  print(f"lower_bound of {x}:",lower_bound)
  print(f"upper_bound of {x}:",upper_bound)
  df=df[(df[x]>=lower_bound) & (df[x]<=upper_bound)]

#vizual representation of the dataset after removing outliers

import matplotlib.pyplot as plt

for x in df.columns:
  if df[x].dtypes=='object' or x=='charges':
    continue
  else:
    sns.boxplot(df[x])
    plt.xlabel(x)
    plt.show()

"""**Encoding**"""

#Encoding - the process of converting categorical (non-numerical) data into a numerical format that machine learning algorithms can understand and process
#Common Encoding Techniques
# 1.One-Hot Encoding
# 2.Label Encoding
# 3.Ordinal Encoding
# 4.Mean Encoding
# 5.Frequency Encoding

#Label Encoding - Assigns a unique numerical label to each category
#We have to import Labelencoding sklearn.preprocessing

from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()

for x in df.columns:
  if df[x].dtype=='object':
    df[x]=le.fit_transform(df[x])

df

df.info()

"""#**VIF**"""

#VIF - Variation Inflation Factor
#The Variance Inflation Factor (VIF) is a measure of multicollinearity,
#which is a situation where multiple independent variables are highly correlated(depended) in a regression model

x=df.drop('y',axis=1)
y=df['y']

x.columns

#importing VIF

from statsmodels.stats.outliers_influence import variance_inflation_factor

#creating a datafrmame

vif_df=pd.DataFrame()
vif_df['features']=x.columns

vif_df

vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_values

vif_df['Multicollinearity']=vif_values

vif_df

x.drop('nr.employed',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

x.drop('cons.price.idx',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

x.drop('pdays',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

x.drop('euribor3m',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

x.drop('cons.conf.idx',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

x.drop('age',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

x.drop('poutcome',axis=1,inplace=True)
x

vif_df=pd.DataFrame()
vif_df['features']=x.columns
vif_values=[]
for i in range(len(x.columns)):
  vif=variance_inflation_factor(x.values,i)
  vif_values.append(vif)

vif_df['Multicollinearity']=vif_values
vif_df

y

"""#**Model Building**"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

x_train,x_test,y_train,y_test=train_test_split(x,y,train_size=0.70)

logistic_model=LogisticRegression()

logistic_model.fit(x_train,y_train)

y_pred=logistic_model.predict(x_test)
y_pred

y_test

"""**Accuracy Score**"""

from sklearn.metrics import accuracy_score

accuracy_score(y_test,y_pred)*100        # Total_True_predictions % Total_no_of_predictions

from sklearn.metrics import confusion_matrix,classification_report
confusion_matrix(y_test,y_pred)